# /src/TriAgent/config/tasks.yaml
# Define tasks one by one --> Most time spent here


B1: # Initial dialogue with user to clarify problem
  description: >
    Engage in dialogue with the user to clarify their exact problem statement, including:
      - Patient constraints,
      - Demographics,
      - Patient medical data,
      - Current biomarkers, readings and lab results already available for the patient
  expected_output: >
    A report of the user's query including the constraints and specifity.


Communicate_DatSci: # Use info from dialogue to pass relevant info to data scientist
  context: [B1]
  description: >
    Use the context to communicate the current problem and information to a data scientist.
    They will use this information to search databases and analyse the data
    Clarify the clinical data to perform data analysis
  expected_output: >
    A report using the clinical data that informs a Data Scientist on the relevant features to analyse.
    This report should be able to inform a data scientist how to run the following.
    1. Feature Engineering
    2. Data preprocessing
    3. Machine Learning model training

Data_Synthesis: # Requires a generative model as a tool: Requires dynamically defining features
  context: [B1]
  description: >
    Use the context to generate a dataset with relevant features.
    Leverage the generative model to do so, with relevant features.
  expected_output: >
    A CSV file with a few thousand rows of synthetic patient data.
    This patient data contains all sorts of biomarkers and relevant diseases with their ICD codes

EDA: # Exploratory data analysis
  context: [Communicate_DatSci]
  description: >
    Perform Exploratory Data Analysis on the data. 
    Obtain summary statistics to obtain various metrics that are relevant to our problem.
  expected_output: >
    Summary of data distributions and descriptive statistics. Detailed enough to run an AutoML pipeline
  


AutoML: # Run AutoML pipeline. May require extra LLM layer/task to interpret requirements from EDA and Communicate_DatSci.
  context: [EDA, Communicate_DatSci]
  description: >
    Run an AutoML tool on the data with the knowledge we have from the patient.
    Find the best ML model to fit the data, and return the weights of each biomarker found from the dataset
  expected_output: >
    The most optimal ML model for the data  according to the AutoML pipeline.
    Also a set of the candidate biomarkers that could help predict the target feature.
    Include each biomarker's corresponding importance score according to the model.


B2: # Generate statistical report of each biomarker in predicting the target feature
  context: [AutoML]
  description: >
    Generate a report of the most significant features that determine the output of the model. 
    Also include their respective weights.
  expected_output: >
    A list of the most descriptive biomarkers to predict the target feature:
      1. Biomarker 1: Weight 1
      2. Biomarker 2: Weight 2
      .
      .
      .
      N. Biomarker N: Weight N


Plan_Research: # Generate queries for research
  context: [B1, B2]
  description: >
    Using the provided context, create a suitable query for each biomarker.
    This query will be used to perform RAG and collect clinical, academic evidence from our knowledge base
  expected_output: >
    Array of strings with each string being a query used to search a knowledge base.
    ["query 1", "query 2", ..., "query N"] for N queries/biomarkers.



Research: # Requires RAG capability and access to knowledge base
  context: [Plan_Research]
  description: >
    Using the research brief, research each query in the knowledge base to find the best fits across the vector database
  expected_output: >
    JSON in the structure of
      {
        "query": [
          {"evidence": text_excerpt, "embedding": vector_embedding_of_text_excerpt},
          {"evidence": text_excerpt, "embedding": vector_embedding_of_text_excerpt},...
      ]
    }


Clean_Research: # Use LLM to examine evidence for each query and deduplicate
  context: [Research]
  description: >
    Deduplicate the JSON from any evidence or queries that are shared. Clean and tidy it up from any irrelevant points.
  expected_output: >
    JSON in the structure of
      {
        "query": [
          {"evidence": text_excerpt, "embedding": vector_embedding_of_text_excerpt},
          {"evidence": text_excerpt, "embedding": vector_embedding_of_text_excerpt},...
      ]
    }


Compute_Similarity: # Manually defined custom tool to convert research JSON to tensor. Requires PyTorch
  context: [Clean_Research]
  description: >
    Generate similarity matrix from the JSON
  expected_output: >
    Tensor


Evaluate_Research: # Generate a report given the evidence for each biomarker
  context: [B2, Clean_Research, Compute_Similarity]
  description: >
    Inspect the similarity matrix and split them into 2 groups: 
    1. Biomarkers with established evidence
    2. Biomarkers with novel evidence
    Use the evidence from the Similarity Matrix {Compute_Similarity} and the statistical knowledge {B2}.
    Also use the actual text in {Clean_Research} to confirm this.
  expected_output: >
    Markdown report with 2 sections: 1 for each category of evidence in support of the biomarker.
    Provide the following for each biomarker:
      1. The name of the biomarker.
      2. Statistical evidence in support of the biomarker - The weighting of the feature.
      3. Academic evidence in support of the biomarker - Quotations directly from the text.

